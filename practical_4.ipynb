{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DhU7d8MT6Y4Y"
      },
      "outputs": [],
      "source": [
        " # Step 1: Import the NLTK library\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download the punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Alternative for newer NLTK versions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BrPj4bo60Fw",
        "outputId": "b2bf737b-6364-4ef6-f301-18b2c18b4946"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load a sample text corpus\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing (NLP) is a subfield of artificial intelligence\n",
        "that focuses on the interaction between computers and human language.\n",
        "It enables computers to understand, interpret, and generate human language.\n",
        "NLP applications include machine translation, sentiment analysis,\n",
        "chatbots, and speech recognition. Language models are fundamental to NLP.\n",
        "\"\"\"\n",
        "print(\"Sample Text Corpus:\")\n",
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-IWgaOu66CF",
        "outputId": "9e877c3f-d3fc-454c-a0ca-124012995b91"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Text Corpus:\n",
            "\n",
            "Natural Language Processing (NLP) is a subfield of artificial intelligence \n",
            "that focuses on the interaction between computers and human language. \n",
            "It enables computers to understand, interpret, and generate human language. \n",
            "NLP applications include machine translation, sentiment analysis, \n",
            "chatbots, and speech recognition. Language models are fundamental to NLP.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenize the text into words\n",
        "words = word_tokenize(sample_text.lower())\n",
        "print(\"\\nTokenized Words:\")\n",
        "print(words)\n",
        "print(f\"Total tokens: {len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HI_JBIK7D_N",
        "outputId": "8d922d6d-427b-47c7-f9ef-581ae17380a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Words:\n",
            "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', '.', 'it', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'nlp', 'applications', 'include', 'machine', 'translation', ',', 'sentiment', 'analysis', ',', 'chatbots', ',', 'and', 'speech', 'recognition', '.', 'language', 'models', 'are', 'fundamental', 'to', 'nlp', '.']\n",
            "Total tokens: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate N-grams using nltk.ngrams()\n",
        "# Let's generate 2-grams (bigrams) as an example\n",
        "n = 2  # You can change this to 3 for trigrams, 4 for 4-grams, etc.\n",
        "generated_ngrams = list(ngrams(words, n))\n",
        "print(f\"\\nGenerated {n}-grams:\")\n",
        "for i, gram in enumerate(generated_ngrams[:10]):  # Show first 10\n",
        "    print(f\"{i+1}. {gram}\")\n",
        "print(f\"Total {n}-grams generated: {len(generated_ngrams)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrbXZxd67ITK",
        "outputId": "a30d0477-945e-47c8-a656-c7f322d5c1e3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated 2-grams:\n",
            "1. ('natural', 'language')\n",
            "2. ('language', 'processing')\n",
            "3. ('processing', '(')\n",
            "4. ('(', 'nlp')\n",
            "5. ('nlp', ')')\n",
            "6. (')', 'is')\n",
            "7. ('is', 'a')\n",
            "8. ('a', 'subfield')\n",
            "9. ('subfield', 'of')\n",
            "10. ('of', 'artificial')\n",
            "Total 2-grams generated: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Count frequency of each N-gram\n",
        "ngram_freq = Counter(generated_ngrams)\n",
        "print(f\"\\nFrequency of {n}-grams:\")\n",
        "for gram, freq in list(ngram_freq.items())[:10]:  # Show top 10\n",
        "    print(f\"{gram}: {freq} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVkQEHsP7OxY",
        "outputId": "80447f38-79f5-4621-cd7e-5428dfbc4e3f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequency of 2-grams:\n",
            "('natural', 'language'): 1 times\n",
            "('language', 'processing'): 1 times\n",
            "('processing', '('): 1 times\n",
            "('(', 'nlp'): 1 times\n",
            "('nlp', ')'): 1 times\n",
            "(')', 'is'): 1 times\n",
            "('is', 'a'): 1 times\n",
            "('a', 'subfield'): 1 times\n",
            "('subfield', 'of'): 1 times\n",
            "('of', 'artificial'): 1 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Display the most common N-grams\n",
        "most_common_ngrams = ngram_freq.most_common(10)\n",
        "print(f\"\\nTop 10 Most Common {n}-grams:\")\n",
        "for i, (gram, freq) in enumerate(most_common_ngrams, 1):\n",
        "    print(f\"{i}. {gram}: {freq} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daBZLNwP7Wb9",
        "outputId": "35b6ceb6-4544-40f2-c798-cd58eaa23d59"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Most Common 2-grams:\n",
            "1. ('human', 'language'): 2 times\n",
            "2. ('language', '.'): 2 times\n",
            "3. (',', 'and'): 2 times\n",
            "4. ('natural', 'language'): 1 times\n",
            "5. ('language', 'processing'): 1 times\n",
            "6. ('processing', '('): 1 times\n",
            "7. ('(', 'nlp'): 1 times\n",
            "8. ('nlp', ')'): 1 times\n",
            "9. (')', 'is'): 1 times\n",
            "10. ('is', 'a'): 1 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Analyze text patterns\n",
        "print(\"\\nText Pattern Analysis:\")\n",
        "print(f\"1. Most frequent {n}-gram: {most_common_ngrams[0][0]} ({most_common_ngrams[0][1]} times)\")\n",
        "print(f\"2. Unique {n}-grams: {len(ngram_freq)}\")\n",
        "print(f\"3. Total {n}-grams: {sum(ngram_freq.values())}\")\n",
        "\n",
        "# Calculate basic probabilities\n",
        "print(\"\\nBasic Probability Analysis:\")\n",
        "for gram, freq in most_common_ngrams[:3]:\n",
        "    probability = freq / len(generated_ngrams)\n",
        "    print(f\"P({gram}) = {freq}/{len(generated_ngrams)} = {probability:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTji-fhB7YEx",
        "outputId": "d1fccf99-b29d-42f6-c21f-5b6937133bd4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text Pattern Analysis:\n",
            "1. Most frequent 2-gram: ('human', 'language') (2 times)\n",
            "2. Unique 2-grams: 54\n",
            "3. Total 2-grams: 57\n",
            "\n",
            "Basic Probability Analysis:\n",
            "P(('human', 'language')) = 2/57 = 0.0351\n",
            "P(('language', '.')) = 2/57 = 0.0351\n",
            "P((',', 'and')) = 2/57 = 0.0351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save results for NLP applications\n",
        "import json\n",
        "\n",
        "# Save n-grams and their frequencies to a file\n",
        "results = {\n",
        "    'text_corpus': sample_text,\n",
        "    'tokens': words,\n",
        "    'n_value': n,\n",
        "    'ngrams_frequencies': {str(k): v for k, v in ngram_freq.items()},\n",
        "    'most_common': [(str(k), v) for k, v in most_common_ngrams]\n",
        "}\n",
        "\n",
        "with open('ngrams_language_model.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(\"\\nResults saved to 'ngrams_language_model.json'\")\n",
        "\n",
        "# Also save as text file for readability\n",
        "with open('ngrams_language_model.txt', 'w') as f:\n",
        "    f.write(\"N-GRAMS AND LANGUAGE MODELING RESULTS\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    f.write(f\"Sample Text: {sample_text}\\n\\n\")\n",
        "    f.write(f\"Token Count: {len(words)}\\n\")\n",
        "    f.write(f\"N-gram Type: {n}-grams\\n\")\n",
        "    f.write(f\"Total {n}-grams: {len(generated_ngrams)}\\n\")\n",
        "    f.write(f\"Unique {n}-grams: {len(ngram_freq)}\\n\\n\")\n",
        "    f.write(\"Top 10 Most Common N-grams:\\n\")\n",
        "    for i, (gram, freq) in enumerate(most_common_ngrams, 1):\n",
        "        f.write(f\"{i}. {gram}: {freq} times\\n\")\n",
        "\n",
        "print(\"Results saved to 'ngrams_language_model.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzdnzVJN7dTv",
        "outputId": "47063396-61e8-4b80-a6d9-e4deeb3897c4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results saved to 'ngrams_language_model.json'\n",
            "Results saved to 'ngrams_language_model.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRACTICAL 4 (ii): Unigrams, Bigrams and Trigrams\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRACTICAL 4 (ii): Unigrams, Bigrams and Trigrams\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Import NLTK and download the punkt dataset\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download punkt if not already downloaded\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5XJuaAQ7llf",
        "outputId": "307986bd-8f62-4a3b-aadc-65de76986e2e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PRACTICAL 4 (ii): Unigrams, Bigrams and Trigrams\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load a sample corpus\n",
        "corpus_text = \"\"\"\n",
        "Language modeling is a fundamental task in natural language processing.\n",
        "Models predict the probability of word sequences. Unigrams consider single words.\n",
        "Bigrams consider pairs of words. Trigrams consider three words together.\n",
        "N-grams help in text generation, speech recognition, and machine translation.\n",
        "\"\"\"\n",
        "print(\"Sample Corpus:\")\n",
        "print(corpus_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsCHbl5v7tIU",
        "outputId": "d30e5995-ba6c-4ec7-82c1-35450aa29c65"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Corpus:\n",
            "\n",
            "Language modeling is a fundamental task in natural language processing. \n",
            "Models predict the probability of word sequences. Unigrams consider single words. \n",
            "Bigrams consider pairs of words. Trigrams consider three words together.\n",
            "N-grams help in text generation, speech recognition, and machine translation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Tokenize text\n",
        "tokens = word_tokenize(corpus_text.lower())\n",
        "print(\"\\nTokenized Text:\")\n",
        "print(tokens)\n",
        "print(f\"Total tokens: {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP__3pqk7xAH",
        "outputId": "88bbd2c8-93e9-4e0d-9116-35c7d6b7eac6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Text:\n",
            "['language', 'modeling', 'is', 'a', 'fundamental', 'task', 'in', 'natural', 'language', 'processing', '.', 'models', 'predict', 'the', 'probability', 'of', 'word', 'sequences', '.', 'unigrams', 'consider', 'single', 'words', '.', 'bigrams', 'consider', 'pairs', 'of', 'words', '.', 'trigrams', 'consider', 'three', 'words', 'together', '.', 'n-grams', 'help', 'in', 'text', 'generation', ',', 'speech', 'recognition', ',', 'and', 'machine', 'translation', '.']\n",
            "Total tokens: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate unigrams, bigrams and trigrams\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "print(\"\\nGenerated N-grams:\")\n",
        "print(f\"Unigrams: {len(unigrams)} (showing first 10: {unigrams[:10]})\")\n",
        "print(f\"Bigrams: {len(bigrams)} (showing first 10: {bigrams[:10]})\")\n",
        "print(f\"Trigrams: {len(trigrams)} (showing first 10: {trigrams[:10]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aPWK1dq72WE",
        "outputId": "4d91bcdd-d42c-4785-82fa-9b7fd3704178"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated N-grams:\n",
            "Unigrams: 49 (showing first 10: [('language',), ('modeling',), ('is',), ('a',), ('fundamental',), ('task',), ('in',), ('natural',), ('language',), ('processing',)])\n",
            "Bigrams: 48 (showing first 10: [('language', 'modeling'), ('modeling', 'is'), ('is', 'a'), ('a', 'fundamental'), ('fundamental', 'task'), ('task', 'in'), ('in', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', '.')])\n",
            "Trigrams: 47 (showing first 10: [('language', 'modeling', 'is'), ('modeling', 'is', 'a'), ('is', 'a', 'fundamental'), ('a', 'fundamental', 'task'), ('fundamental', 'task', 'in'), ('task', 'in', 'natural'), ('in', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', '.'), ('processing', '.', 'models')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Compute their frequency distribution\n",
        "unigram_freq = Counter(unigrams)\n",
        "bigram_freq = Counter(bigrams)\n",
        "trigram_freq = Counter(trigrams)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(f\"Unique Unigrams: {len(unigram_freq)}\")\n",
        "print(f\"Unique Bigrams: {len(bigram_freq)}\")\n",
        "print(f\"Unique Trigrams: {len(trigram_freq)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9mrE7Ma72Ao",
        "outputId": "54b9c552-182a-4af9-909e-0f2fcce9892c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequency Distribution:\n",
            "Unique Unigrams: 36\n",
            "Unique Bigrams: 47\n",
            "Unique Trigrams: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Display most common N-grams\n",
        "print(\"\\nMOST COMMON N-GRAMS:\")\n",
        "print(\"\\nTop 5 Unigrams:\")\n",
        "for i, (gram, freq) in enumerate(unigram_freq.most_common(5), 1):\n",
        "    print(f\"{i}. {gram[0]}: {freq} times\")\n",
        "\n",
        "print(\"\\nTop 5 Bigrams:\")\n",
        "for i, (gram, freq) in enumerate(bigram_freq.most_common(5), 1):\n",
        "    print(f\"{i}. {gram}: {freq} times\")\n",
        "\n",
        "print(\"\\nTop 5 Trigrams:\")\n",
        "for i, (gram, freq) in enumerate(trigram_freq.most_common(5), 1):\n",
        "    print(f\"{i}. {gram}: {freq} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HH0xyNZ71LD",
        "outputId": "ce27d838-e04c-4284-b4d3-075c1cd8ea4d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MOST COMMON N-GRAMS:\n",
            "\n",
            "Top 5 Unigrams:\n",
            "1. .: 6 times\n",
            "2. consider: 3 times\n",
            "3. words: 3 times\n",
            "4. language: 2 times\n",
            "5. in: 2 times\n",
            "\n",
            "Top 5 Bigrams:\n",
            "1. ('words', '.'): 2 times\n",
            "2. ('language', 'modeling'): 1 times\n",
            "3. ('modeling', 'is'): 1 times\n",
            "4. ('is', 'a'): 1 times\n",
            "5. ('a', 'fundamental'): 1 times\n",
            "\n",
            "Top 5 Trigrams:\n",
            "1. ('language', 'modeling', 'is'): 1 times\n",
            "2. ('modeling', 'is', 'a'): 1 times\n",
            "3. ('is', 'a', 'fundamental'): 1 times\n",
            "4. ('a', 'fundamental', 'task'): 1 times\n",
            "5. ('fundamental', 'task', 'in'): 1 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Analyze language structure\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(\"LANGUAGE STRUCTURE ANALYSIS\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Vocabulary analysis\n",
        "vocabulary = set(tokens)\n",
        "print(f\"1. Vocabulary Size: {len(vocabulary)} unique words\")\n",
        "\n",
        "# N-gram coverage analysis\n",
        "total_unigrams = len(unigrams)\n",
        "total_bigrams = len(bigrams)\n",
        "total_trigrams = len(trigrams)\n",
        "\n",
        "print(f\"\\n2. N-gram Statistics:\")\n",
        "print(f\"   Unigrams: {len(unigram_freq)} unique / {total_unigrams} total\")\n",
        "print(f\"   Bigrams: {len(bigram_freq)} unique / {total_bigrams} total\")\n",
        "print(f\"   Trigrams: {len(trigram_freq)} unique / {total_trigrams} total\")\n",
        "\n",
        "# Context window analysis\n",
        "print(f\"\\n3. Context Window Analysis:\")\n",
        "print(f\"   Unigrams show individual word frequency\")\n",
        "print(f\"   Bigrams show word pair relationships\")\n",
        "print(f\"   Trigrams show three-word phrase patterns\")\n",
        "\n",
        "# Example of language patterns\n",
        "print(f\"\\n4. Example Patterns Found:\")\n",
        "print(f\"   Most common unigram: '{unigram_freq.most_common(1)[0][0][0]}'\")\n",
        "print(f\"   Most common bigram: {bigram_freq.most_common(1)[0][0]}\")\n",
        "print(f\"   Most common trigram: {trigram_freq.most_common(1)[0][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7sAkpoZ70yi",
        "outputId": "458e6fdd-ef1b-40a8-badf-3d19c27866a2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "LANGUAGE STRUCTURE ANALYSIS\n",
            "========================================\n",
            "1. Vocabulary Size: 36 unique words\n",
            "\n",
            "2. N-gram Statistics:\n",
            "   Unigrams: 36 unique / 49 total\n",
            "   Bigrams: 47 unique / 48 total\n",
            "   Trigrams: 47 unique / 47 total\n",
            "\n",
            "3. Context Window Analysis:\n",
            "   Unigrams show individual word frequency\n",
            "   Bigrams show word pair relationships\n",
            "   Trigrams show three-word phrase patterns\n",
            "\n",
            "4. Example Patterns Found:\n",
            "   Most common unigram: '.'\n",
            "   Most common bigram: ('words', '.')\n",
            "   Most common trigram: ('language', 'modeling', 'is')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Save results\n",
        "import json\n",
        "\n",
        "# Save all results to a JSON file\n",
        "all_results = {\n",
        "    'corpus': corpus_text,\n",
        "    'tokens': tokens,\n",
        "    'vocabulary_size': len(vocabulary),\n",
        "    'unigrams': {\n",
        "        'total': total_unigrams,\n",
        "        'unique': len(unigram_freq),\n",
        "        'top_10': [(str(k[0]), v) for k, v in unigram_freq.most_common(10)]\n",
        "    },\n",
        "    'bigrams': {\n",
        "        'total': total_bigrams,\n",
        "        'unique': len(bigram_freq),\n",
        "        'top_10': [(str(k), v) for k, v in bigram_freq.most_common(10)]\n",
        "    },\n",
        "    'trigrams': {\n",
        "        'total': total_trigrams,\n",
        "        'unique': len(trigram_freq),\n",
        "        'top_10': [(str(k), v) for k, v in trigram_freq.most_common(10)]\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('unigrams_bigrams_trigrams_results.json', 'w') as f:\n",
        "    json.dump(all_results, f, indent=4)\n",
        "\n",
        "print(\"\\nResults saved to 'unigrams_bigrams_trigrams_results.json'\")\n",
        "\n",
        "# Also create a readable text report\n",
        "with open('ngrams_analysis_report.txt', 'w') as f:\n",
        "    f.write(\"UNIGRAMS, BIGRAMS AND TRIGRAMS ANALYSIS\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    f.write(\"CORPUS TEXT:\\n\")\n",
        "    f.write(corpus_text + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"TOKEN ANALYSIS:\\n\")\n",
        "    f.write(f\"Total tokens: {len(tokens)}\\n\")\n",
        "    f.write(f\"Vocabulary size: {len(vocabulary)} words\\n\\n\")\n",
        "\n",
        "    f.write(\"UNIGRAMS ANALYSIS:\\n\")\n",
        "    f.write(f\"Total unigrams: {total_unigrams}\\n\")\n",
        "    f.write(f\"Unique unigrams: {len(unigram_freq)}\\n\")\n",
        "    f.write(\"Top 10 Unigrams:\\n\")\n",
        "    for i, (gram, freq) in enumerate(unigram_freq.most_common(10), 1):\n",
        "        f.write(f\"  {i}. {gram[0]}: {freq} times\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"BIGRAMS ANALYSIS:\\n\")\n",
        "    f.write(f\"Total bigrams: {total_bigrams}\\n\")\n",
        "    f.write(f\"Unique bigrams: {len(bigram_freq)}\\n\")\n",
        "    f.write(\"Top 10 Bigrams:\\n\")\n",
        "    for i, (gram, freq) in enumerate(bigram_freq.most_common(10), 1):\n",
        "        f.write(f\"  {i}. {gram}: {freq} times\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"TRIGRAMS ANALYSIS:\\n\")\n",
        "    f.write(f\"Total trigrams: {total_trigrams}\\n\")\n",
        "    f.write(f\"Unique trigrams: {len(trigram_freq)}\\n\")\n",
        "    f.write(\"Top 10 Trigrams:\\n\")\n",
        "    for i, (gram, freq) in enumerate(trigram_freq.most_common(10), 1):\n",
        "        f.write(f\"  {i}. {gram}: {freq} times\\n\")\n",
        "\n",
        "print(\"Report saved to 'ngrams_analysis_report.txt'\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRACTICAL 4 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DldS0vz70Oa",
        "outputId": "d7845fd2-d053-430d-8ade-fad8bca4a81a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results saved to 'unigrams_bigrams_trigrams_results.json'\n",
            "Report saved to 'ngrams_analysis_report.txt'\n",
            "\n",
            "============================================================\n",
            "PRACTICAL 4 COMPLETED SUCCESSFULLY!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}